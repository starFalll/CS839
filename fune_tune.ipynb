{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fune-tunning Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
      "Collecting auto-gptq\n",
      "  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.7.1%2Bcu118-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in ./llama2/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./llama2/lib/python3.10/site-packages (from auto-gptq) (2.2.1)\n",
      "Requirement already satisfied: datasets in ./llama2/lib/python3.10/site-packages (from auto-gptq) (2.18.0)\n",
      "Requirement already satisfied: transformers>=4.31.0 in ./llama2/lib/python3.10/site-packages (from auto-gptq) (4.38.2)\n",
      "Requirement already satisfied: safetensors in ./llama2/lib/python3.10/site-packages (from auto-gptq) (0.4.2)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in ./llama2/lib/python3.10/site-packages (from auto-gptq) (0.28.0)\n",
      "Requirement already satisfied: peft>=0.5.0 in ./llama2/lib/python3.10/site-packages (from auto-gptq) (0.10.0)\n",
      "Requirement already satisfied: numpy in ./llama2/lib/python3.10/site-packages (from auto-gptq) (1.26.4)\n",
      "Collecting gekko\n",
      "  Downloading gekko-1.1.0-py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: tqdm in ./llama2/lib/python3.10/site-packages (from auto-gptq) (4.66.2)\n",
      "Requirement already satisfied: huggingface-hub in ./llama2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (0.21.4)\n",
      "Requirement already satisfied: pyyaml in ./llama2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./llama2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (24.0)\n",
      "Requirement already satisfied: psutil in ./llama2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.8)\n",
      "Requirement already satisfied: filelock in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
      "Requirement already satisfied: fsspec in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.10.0)\n",
      "Requirement already satisfied: jinja2 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.0.2.54)\n",
      "Requirement already satisfied: triton==2.2.0 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.2.0)\n",
      "Requirement already satisfied: sympy in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./llama2/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq) (12.4.99)\n",
      "Requirement already satisfied: requests in ./llama2/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./llama2/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./llama2/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (3.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (0.6)\n",
      "Requirement already satisfied: pandas in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (2.2.1)\n",
      "Requirement already satisfied: aiohttp in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (3.9.3)\n",
      "Requirement already satisfied: multiprocess in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.16)\n",
      "Requirement already satisfied: six in ./llama2/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./llama2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./llama2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./llama2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./llama2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./llama2/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./llama2/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./llama2/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./llama2/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./llama2/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Installing collected packages: rouge, gekko, auto-gptq\n",
      "Successfully installed auto-gptq-0.7.1+cu118 gekko-1.1.0 rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers>=4.32.0 optimum>=1.12.0\n",
    "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/s/d/sdam/miniconda3/envs/llama2/lib/python3.12/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-70B-chat-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     32\u001b[0m prompt_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m[INST] <<SYS>>\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124mYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer to a question, please don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt share false information.\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124mAll your answers should be in the format of a list, such as [col1,col2,...].\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m*** Generate:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(prompt_template, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     43\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs\u001b[38;5;241m=\u001b[39minput_ids, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Column Names MUST be limited to the following:\n",
    "name, description, team, type, age, location, year, city, rank, status, state, category,\n",
    "weight, code, club, artist, result, position, country, notes, class, company, album, symbol,\n",
    "address, duration, format, county, day, gender, industry, language, sex, product, jockey,\n",
    "region, area, service, teamName, order, isbn, fileSize, grades, publisher, plays, origin,\n",
    "elevation, affiliation, component, owner, genre,  manufacturer, brand, family, credit, depth,\n",
    "classification, collection, species, command, nationality, currency, range, affiliate,\n",
    "birthDate, ranking, capacity, birthPlace, person, creator, operator, religion, education,\n",
    "requirement, director, sales, continent, organisation\n",
    "Do not use any column names aside from these.\n",
    "\n",
    "Given the following relational table:\n",
    "\n",
    "IDI1,isopentenyl-diphosphate delta isomerase..isopentenyl-diphosphate delta isomerase.\n",
    "FDFT1,farnesyl-diphosphate farnesyltransferase 1.DGPT.farnesyl-diphosphate farnesyltransferase 1.\n",
    "FDPS,\"farnesyl diphosphate synthase (farnesyl pyrophosphate synthetase, dimethylallyltranstransferase, geranyltranstransferase).FPS..\"\n",
    "HPCAL1,hippocalcin-like 1.BDR1; HLP2; VILIP-3.hippocalcin-like 1.\n",
    "LRP8,\"low density lipoprotein receptor-related protein 8, apolipoprotein e receptor.APOER2; HSZ75190.low density lipoprotein receptor-related protein 8 isoform 3 precursor.\"\n",
    "AP2S1,\"adaptor-related protein complex 2, sigma 1 subunit.AP17; CLAPS2; AP17-DELTA.adaptor-related protein complex 2, sigma 1 subunit isoform AP17delta.\"\n",
    "SQLE,squalene epoxidase..squalene monooxygenase.\n",
    "HPCA,hippocalcin.BDR2.hippocalcin.\n",
    ",\"tubulin, beta, 2.\"\n",
    "AP3S1,\"adaptor-related protein complex 3, sigma 1 subunit.CLAPS3; Sigma3A.adaptor-related protein complex 3, sigma 1 subunit.\"\n",
    "AP1S2,\"adaptor-related protein complex 1, sigma 2 subunit.DC22; SIGMA1B; MGC:1902.adaptor-related protein complex 1 sigma 2 subunit.\"\n",
    "TUBA1,\"tubulin, alpha 1 (testis specific).FLJ30169; H2-ALPHA.tubulin, alpha 1.\"\n",
    "AP2A1,\"adaptor-related protein complex 2, alpha 1 subunit.ADTAA; CLAPA1; AP2-ALPHA.adaptor-related protein complex 2, alpha 1 subunit isoform 2.\"\n",
    "SEC24D,\"SEC24 related gene family, member D (S. cerevisiae).KIAA0755.Sec24-related protein D.\"\n",
    "\n",
    "Guess the column names for the whole table. There are only 2 columns in the table. Give only 1 answer for each column.\n",
    "\"\"\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "All your answers should be in the format of a list, such as [col1,col2,...].\n",
    "<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=4096)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from json import loads\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:08<00:00,  3.68s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Column Names are limited to the following:\n",
    "    name, description, team, type, age, location, year, city, rank, status, state, category,\n",
    "    weight, code, club, artist, result, position, country, notes, class, company, album, symbol,\n",
    "    address, duration, format, county, day, gender, industry, language, sex, product, jockey,\n",
    "    region, area, service, teamName, order, isbn, fileSize, grades, publisher, plays, origin,\n",
    "    elevation, affiliation, component, owner, genre,  manufacturer, brand, family, credit, depth,\n",
    "    classification, collection, species, command, nationality, currency, range, affiliate,\n",
    "    birthDate, ranking, capacity, birthPlace, person, creator, operator, religion, education,\n",
    "    requirement, director, sales, continent, organisation\n",
    "    Do not use any column names aside from these.\n",
    "\n",
    "    Output must be in valid JSON like the following example {\"colnames\" : [\"col1\", \"col2\"]}\n",
    "    \"\"\"\n",
    "\n",
    "tabledir = \"K4\" # Note that this doesn't contain '/' at the end\n",
    "filenames = os.listdir(tabledir + '/')\n",
    "real_cols = []\n",
    "pred_cols = []\n",
    "\n",
    "for filename in tqdm(filenames[0:100]):\n",
    "    with open(tabledir + '/' + filename) as f:\n",
    "        linelist = f.readlines()\n",
    "        colnames = linelist[0][:-1].split(',')\n",
    "        real_cols += colnames\n",
    "\n",
    "        prompt_template=f'''[INST] <<SYS>>\n",
    "You are a database expert who can make general predictions for missing column values in database tables, and the predicted column names are within the required candidate set. All output must be in valid JSON. Don't add explanation beyond the JSON.\n",
    "Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{prompt}\n",
    "Given the following relational table:\n",
    "{''.join(linelist[1:21])}\n",
    "Guess the column names for the whole table. There are only {len(colnames)} columns in the table. It is possible for multiple columns to have the same name\n",
    "[/INST]\n",
    "'''\n",
    "\n",
    "        #print(\"*** Generate:\")\n",
    "        input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "        output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=4096)\n",
    "        #print(tokenizer.decode(output[0]).replace('\\n',' '))\n",
    "        pred_tmp = loads(tokenizer.decode(output[0]).split('[/INST]')[1].split('</s>')[0].replace('\\n',''))[\"colnames\"]\n",
    "        if len(colnames) == len(pred_tmp): # TODO: add an else statement\n",
    "            pred_cols += pred_tmp\n",
    "        else:\n",
    "            pred_cols += [\"???\"] * len(colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tabledir + '_true.npy', 'wb') as f:\n",
    "    np.save(f, np.array(real_cols, dtype='<U14'))\n",
    "with open(tabledir + '_pred.npy', 'wb') as f:\n",
    "    np.save(f, np.array(pred_cols, dtype='<U14'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [12:34<00:00,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Column Names are limited to the following:\n",
    "    name, description, team, type, age, location, year, city, rank, status, state, category,\n",
    "    weight, code, club, artist, result, position, country, notes, class, company, album, symbol,\n",
    "    address, duration, format, county, day, gender, industry, language, sex, product, jockey,\n",
    "    region, area, service, teamName, order, isbn, fileSize, grades, publisher, plays, origin,\n",
    "    elevation, affiliation, component, owner, genre,  manufacturer, brand, family, credit, depth,\n",
    "    classification, collection, species, command, nationality, currency, range, affiliate,\n",
    "    birthDate, ranking, capacity, birthPlace, person, creator, operator, religion, education,\n",
    "    requirement, director, sales, continent, organisation\n",
    "    Do not use any column names aside from these.\n",
    "\n",
    "    Output must be in valid JSON like the following example {\"column\" : \"col1\"}. Give only 1 prediction. Do NOT add any explanation beyond the JSON.\n",
    "    \"\"\"\n",
    "\n",
    "tabledir = \"K0\" # Note that this doesn't contain '/' at the end\n",
    "filenames = os.listdir(tabledir + '/')\n",
    "real_cols_single = []\n",
    "pred_cols_single = []\n",
    "\n",
    "for filename in tqdm(filenames[0:100]):\n",
    "    fullpath = tabledir + '/' + filename\n",
    "    with open(fullpath) as f:\n",
    "        linelist = f.readlines()\n",
    "        colnames = linelist[0][:-1].split(',')\n",
    "        real_cols_single += colnames\n",
    "\n",
    "        df = pd.read_csv(fullpath).astype(str)\n",
    "        for col in df.columns:\n",
    "            prompt_template=f'''[INST] <<SYS>>\n",
    "You are a database expert who can make general predictions for missing column values in database tables, and the predicted column names are within the required candidate set. All output must be in valid JSON. Do NOT add explanation beyond the JSON.\n",
    "Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{prompt}\n",
    "Given the following column values in a relational table: {', '.join(df[col][0:20])}\n",
    "Guess the column name\n",
    "[/INST]\n",
    "'''\n",
    "\n",
    "            #print(\"*** Generate:\")\n",
    "            input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "            output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=4096)\n",
    "            #print(tokenizer.decode(output[0]).replace('\\n',' '))\n",
    "            pred_cols_single.append(loads('{' + tokenizer.decode(output[0]).split('[/INST]')[1].split('</s>')[0].replace('\\n','').replace('}','{').split('{')[1] + '}')[\"column\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tabledir + '_true_single.npy', 'wb') as f:\n",
    "    np.save(f, np.array(real_cols_single, dtype='<U14'))\n",
    "with open(tabledir + '_pred_single.npy', 'wb') as f:\n",
    "    np.save(f, np.array(pred_cols_single, dtype='<U14'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
