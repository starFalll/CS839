{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fune-tunning Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
      "Collecting auto-gptq\n",
      "  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.7.1%2Bcu118-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in ./llama2/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./llama2/lib/python3.10/site-packages (from auto-gptq) (2.2.1)\n",
      "Requirement already satisfied: datasets in ./llama2/lib/python3.10/site-packages (from auto-gptq) (2.18.0)\n",
      "Requirement already satisfied: transformers>=4.31.0 in ./llama2/lib/python3.10/site-packages (from auto-gptq) (4.38.2)\n",
      "Requirement already satisfied: safetensors in ./llama2/lib/python3.10/site-packages (from auto-gptq) (0.4.2)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in ./llama2/lib/python3.10/site-packages (from auto-gptq) (0.28.0)\n",
      "Requirement already satisfied: peft>=0.5.0 in ./llama2/lib/python3.10/site-packages (from auto-gptq) (0.10.0)\n",
      "Requirement already satisfied: numpy in ./llama2/lib/python3.10/site-packages (from auto-gptq) (1.26.4)\n",
      "Collecting gekko\n",
      "  Downloading gekko-1.1.0-py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: tqdm in ./llama2/lib/python3.10/site-packages (from auto-gptq) (4.66.2)\n",
      "Requirement already satisfied: huggingface-hub in ./llama2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (0.21.4)\n",
      "Requirement already satisfied: pyyaml in ./llama2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./llama2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (24.0)\n",
      "Requirement already satisfied: psutil in ./llama2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.8)\n",
      "Requirement already satisfied: filelock in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
      "Requirement already satisfied: fsspec in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.10.0)\n",
      "Requirement already satisfied: jinja2 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.0.2.54)\n",
      "Requirement already satisfied: triton==2.2.0 in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.2.0)\n",
      "Requirement already satisfied: sympy in ./llama2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./llama2/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq) (12.4.99)\n",
      "Requirement already satisfied: requests in ./llama2/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./llama2/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./llama2/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (3.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (0.6)\n",
      "Requirement already satisfied: pandas in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (2.2.1)\n",
      "Requirement already satisfied: aiohttp in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (3.9.3)\n",
      "Requirement already satisfied: multiprocess in ./llama2/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.16)\n",
      "Requirement already satisfied: six in ./llama2/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./llama2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./llama2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./llama2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./llama2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./llama2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./llama2/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./llama2/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./llama2/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./llama2/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./llama2/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Installing collected packages: rouge, gekko, auto-gptq\n",
      "Successfully installed auto-gptq-0.7.1+cu118 gekko-1.1.0 rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers>=4.32.0 optimum>=1.12.0\n",
    "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecba12705f574927bc7ee3000e9e79a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/35.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/e/t/ethanfang/private/llama2/llama2/lib/python3.10/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1230ff98dd234bc6a4a6280c4c008e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d11efed8e5421e990858eaaa6645d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/745 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a63451a3df4d05a01f2523da2a5e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd39857ec34454a9c355777df31e580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314e7af1197c45d9bc1c26672df38e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-70B-chat-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "Tell me about AI[/INST]\n",
      "\n",
      "Artificial intelligence (AI) refers to the ability of machines or computer programs to mimic or simulate human intelligence, such as learning, problem-solving, decision-making, perception, or language. AI can be applied to various industries and fields, including natural language processing, image and speech recognition, robotics, healthcare, finance, transportation, and education. AI systems can be trained on large datasets to improve their performance and accuracy over time.\n",
      "\n",
      "There are several types of AI, including:\n",
      "\n",
      "1. Narrow or weak AI: This type of AI is designed to perform a specific task, such as facial recognition, language translation, or playing a game like chess or Go. Narrow AI is the most common type of AI and is used in many applications.\n",
      "2. General or strong AI: This type of AI is designed to perform any intellectual task that a human can do. General AI has the ability to understand, learn, and apply knowledge across a wide range of tasks, much like a human does. General AI is still a topic of ongoing research and development.\n",
      "3. Superintelligence: This type of AI is significantly more intelligent than the best human minds. Superintelligence could potentially solve complex problems that are unsolvable by humans, but it also raises concerns about safety, control, and ethics.\n",
      "4. Artificial general intelligence (AGI): This type of AI is designed to perform any intellectual task that a human can do, and is considered a long-term goal for AI researchers. AGI would require a system that can learn, understand, and apply knowledge across a wide range of tasks, much like a human does.\n",
      "\n",
      "AI has many potential benefits, such as improving efficiency, productivity, and decision-making, and helping to solve complex problems. However, AI also raises ethical and societal concerns, such as privacy, bias, job displacement, and the potential for AI systems to be misused. It's important to ensure that AI is developed and used in a responsible, ethical, and transparent manner.</s>\n",
      "*** Pipeline:\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "Tell me about AI[/INST]\n",
      "\n",
      "Artificial intelligence (AI) refers to the ability of machines or computer programs to mimic intelligent human behavior, such as learning, problem-solving, decision-making, perception, and language understanding. AI systems use algorithms and data to make decisions, classify objects, and generate insights based on large amounts of information.\n",
      "\n",
      "There are many different types of AI, including:\n",
      "\n",
      "1. Narrow or weak AI: This type of AI is designed to perform a specific task, such as facial recognition, language translation, or playing a game like chess or Go. Narrow AI is the most common type of AI and is used in many applications today.\n",
      "2. General or strong AI: This type of AI is designed to perform any intellectual task that a human can do. General AI has the ability to reason, learn, and improve its performance over time. It is still a topic of research and development, and it is considered a long-term goal for AI researchers.\n",
      "3. Superintelligence: This type of AI is significantly more intelligent than the best human minds. Superintelligence could potentially solve complex problems that are currently unsolvable, but it also raises concerns about safety and control.\n",
      "4. Artificial general intelligence (AGI): This type of AI is designed to perform any intellectual task that a human can do, and it is considered a long-term goal for AI researchers. AGI would require a system that can learn, understand, and apply knowledge across a wide range of tasks, much like a human does.\n",
      "5. Cognitive computing: This type of AI is focused on creating machines that can think and learn like humans, using techniques such as machine learning, natural language processing, and computer vision. Cognitive computing is used in applications such as customer service chatbots, image recognition, and expert systems.\n",
      "\n",
      "AI has many potential benefits, such as improving healthcare outcomes, increasing productivity, and helping with complex scientific and mathematical problems. However, AI also raises ethical and societal concerns, such as privacy, bias, job displacement, and ensuring accountability and transparency in decision-making processes.\n",
      "\n",
      "Overall, AI is a rapidly evolving field that has the potential to transform many aspects of our lives, and it is important to consider both the benefits and the challeng\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Column Names are limited to the following:\n",
      "    name, description, team, type, age, location, year, city, rank, status, state, category,\n",
      "    weight, code, club, artist, result, position, country, notes, class, company, album, symbol,\n",
      "    address, duration, format, county, day, gender, industry, language, sex, product, jockey,\n",
      "    region, area, service, teamName, order, isbn, fileSize, grades, publisher, plays, origin,\n",
      "    elevation, affiliation, component, owner, genre,  manufacturer, brand, family, credit, depth,\n",
      "    classification, collection, species, command, nationality, currency, range, affiliate,\n",
      "    birthDate, ranking, capacity, birthPlace, person, creator, operator, religion, education,\n",
      "    requirement, director, sales, continent, organisation\n",
      "    Do not use any column names aside from these.\n",
      "\n",
      "Given the following relational table:\n",
      "\n",
      "col1;col2\n",
      "IDI1,isopentenyl-diphosphate delta isomerase..isopentenyl-diphosphate delta isomerase.\n",
      "FDFT1,farnesyl-diphosphate farnesyltransferase 1.DGPT.farnesyl-diphosphate farnesyltransferase 1.\n",
      "FDPS,\"farnesyl diphosphate synthase (farnesyl pyrophosphate synthetase, dimethylallyltranstransferase, geranyltranstransferase).FPS..\"\n",
      "HPCAL1,hippocalcin-like 1.BDR1; HLP2; VILIP-3.hippocalcin-like 1.\n",
      "LRP8,\"low density lipoprotein receptor-related protein 8, apolipoprotein e receptor.APOER2; HSZ75190.low density lipoprotein receptor-related protein 8 isoform 3 precursor.\"\n",
      "AP2S1,\"adaptor-related protein complex 2, sigma 1 subunit.AP17; CLAPS2; AP17-DELTA.adaptor-related protein complex 2, sigma 1 subunit isoform AP17delta.\"\n",
      "SQLE,squalene epoxidase..squalene monooxygenase.\n",
      "HPCA,hippocalcin.BDR2.hippocalcin.\n",
      ",\"tubulin, beta, 2.\"\n",
      "AP3S1,\"adaptor-related protein complex 3, sigma 1 subunit.CLAPS3; Sigma3A.adaptor-related protein complex 3, sigma 1 subunit.\"\n",
      "AP1S2,\"adaptor-related protein complex 1, sigma 2 subunit.DC22; SIGMA1B; MGC:1902.adaptor-related protein complex 1 sigma 2 subunit.\"\n",
      "TUBA1,\"tubulin, alpha 1 (testis specific).FLJ30169; H2-ALPHA.tubulin, alpha 1.\"\n",
      "AP2A1,\"adaptor-related protein complex 2, alpha 1 subunit.ADTAA; CLAPA1; AP2-ALPHA.adaptor-related protein complex 2, alpha 1 subunit isoform 2.\"\n",
      "SEC24D,\"SEC24 related gene family, member D (S. cerevisiae).KIAA0755.Sec24-related protein D.\"\n",
      "Guess the column names for the whole table. There are only 2 columns in the table.\n",
      "[/INST]\n",
      "\n",
      "Based on the information provided, it appears that the table has two columns, and the column names are not explicitly stated. However, we can make an educated guess about the column names based on the data provided.\n",
      "\n",
      "Column 1: Gene/Protein Name\n",
      "\n",
      "* Isopentenyl-diphosphate delta isomerase\n",
      "* Farnesyl-diphosphate farnesyltransferase 1\n",
      "* Farnesyl diphosphate synthase (farnesyl pyrophosphate synthetase, dimethylallyltranstransferase, geranyltranstransferase)\n",
      "* Hippocalcin-like 1\n",
      "* Low density lipoprotein receptor-related protein 8\n",
      "* Apolipoprotein E receptor\n",
      "* Adaptor-related protein complex 2, sigma 1 subunit\n",
      "* Squalene epoxidase\n",
      "* Hippocalcin\n",
      "* Tubulin, beta, 2\n",
      "* Adaptor-related protein complex 3, sigma 1 subunit\n",
      "* Adaptor-related protein complex 1, sigma 2 subunit\n",
      "* Tubulin, alpha 1 (testis specific)\n",
      "* Adaptor-related protein complex 2, alpha 1 subunit\n",
      "* SEC24 related gene family, member D (S. cerevisiae)\n",
      "\n",
      "Column 2: Description/Function\n",
      "\n",
      "* Isopentenyl-diphosphate delta isomerase\n",
      "* Farnesyl-diphosphate farnesyltransferase 1\n",
      "* Farnesyl diphosphate synthase (farnesyl pyrophosphate synthetase, dimethylallyltranstransferase, geranyltranstransferase)\n",
      "* Hippocalcin-like 1\n",
      "* Low density lipoprotein receptor-related protein 8\n",
      "* Apolipoprotein E receptor\n",
      "* Adaptor-related protein complex 2, sigma 1 subunit\n",
      "* Squalene epoxidase\n",
      "* Hippocalcin\n",
      "* Tubulin, beta, 2\n",
      "* Adaptor-related protein complex 3, sigma 1 subunit\n",
      "* Adaptor-related protein complex 1, sigma 2 subunit\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Column Names are limited to the following:\n",
    "    name, description, team, type, age, location, year, city, rank, status, state, category,\n",
    "    weight, code, club, artist, result, position, country, notes, class, company, album, symbol,\n",
    "    address, duration, format, county, day, gender, industry, language, sex, product, jockey,\n",
    "    region, area, service, teamName, order, isbn, fileSize, grades, publisher, plays, origin,\n",
    "    elevation, affiliation, component, owner, genre,  manufacturer, brand, family, credit, depth,\n",
    "    classification, collection, species, command, nationality, currency, range, affiliate,\n",
    "    birthDate, ranking, capacity, birthPlace, person, creator, operator, religion, education,\n",
    "    requirement, director, sales, continent, organisation\n",
    "    Do not use any column names aside from these.\n",
    "\n",
    "Given the following relational table:\n",
    "\n",
    "col1;col2\n",
    "IDI1,isopentenyl-diphosphate delta isomerase..isopentenyl-diphosphate delta isomerase.\n",
    "FDFT1,farnesyl-diphosphate farnesyltransferase 1.DGPT.farnesyl-diphosphate farnesyltransferase 1.\n",
    "FDPS,\"farnesyl diphosphate synthase (farnesyl pyrophosphate synthetase, dimethylallyltranstransferase, geranyltranstransferase).FPS..\"\n",
    "HPCAL1,hippocalcin-like 1.BDR1; HLP2; VILIP-3.hippocalcin-like 1.\n",
    "LRP8,\"low density lipoprotein receptor-related protein 8, apolipoprotein e receptor.APOER2; HSZ75190.low density lipoprotein receptor-related protein 8 isoform 3 precursor.\"\n",
    "AP2S1,\"adaptor-related protein complex 2, sigma 1 subunit.AP17; CLAPS2; AP17-DELTA.adaptor-related protein complex 2, sigma 1 subunit isoform AP17delta.\"\n",
    "SQLE,squalene epoxidase..squalene monooxygenase.\n",
    "HPCA,hippocalcin.BDR2.hippocalcin.\n",
    ",\"tubulin, beta, 2.\"\n",
    "AP3S1,\"adaptor-related protein complex 3, sigma 1 subunit.CLAPS3; Sigma3A.adaptor-related protein complex 3, sigma 1 subunit.\"\n",
    "AP1S2,\"adaptor-related protein complex 1, sigma 2 subunit.DC22; SIGMA1B; MGC:1902.adaptor-related protein complex 1 sigma 2 subunit.\"\n",
    "TUBA1,\"tubulin, alpha 1 (testis specific).FLJ30169; H2-ALPHA.tubulin, alpha 1.\"\n",
    "AP2A1,\"adaptor-related protein complex 2, alpha 1 subunit.ADTAA; CLAPA1; AP2-ALPHA.adaptor-related protein complex 2, alpha 1 subunit isoform 2.\"\n",
    "SEC24D,\"SEC24 related gene family, member D (S. cerevisiae).KIAA0755.Sec24-related protein D.\"\n",
    "Guess the column names for the whole table. There are only 2 columns in the table.\n",
    "\"\"\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=4096)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
