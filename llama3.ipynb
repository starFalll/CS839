{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model type\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_size = \"8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom parameters\n",
    "column_type = \"single\"                            # single/multi, like for single column type, We only read tables with only one column\n",
    "directory_indexs = [\"K0\", \"K1\", \"K2\", \"K3\", \"K4\"] # directory used for iterate\n",
    "file_num = 100                                    # for each directory, read file_num of column_type files\n",
    "rows_num = 20                                     # for each table, how many rows we need read for prompt\n",
    "max_new_tokens = 4096                             # max token for LLM\n",
    "ignore_mismatch = False                           # if throw out mismatch\n",
    "gpu_device = 2                                    # specify the index of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/e/t/ethanfang/private/llama3/llama3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [16:07<00:00, 241.81s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=gpu_device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from json import loads, JSONDecodeError\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# from importlib import reload\n",
    "# import sys\n",
    "from npy_postprocess import canonical_header\n",
    "from f1_llama import report_gen\n",
    "# print(sys.modules['f1_llama'])\n",
    "# reload(sys.modules['f1_llama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:39<02:38, 39.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 7 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:30<02:17, 45.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 6 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [02:13<01:30, 45.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 10 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [02:55<00:43, 43.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 9 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:37<00:00, 43.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 11 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "template_context = \"\"\"\n",
    "Column Names are limited to the following:\n",
    "name, description, team, type, age, location, year, city, rank, status, state, category,\n",
    "weight, code, club, artist, result, position, country, notes, class, company, album, symbol,\n",
    "address, duration, format, county, day, gender, industry, language, sex, product, jockey,\n",
    "region, area, service, teamName, order, isbn, fileSize, grades, publisher, plays, origin,\n",
    "elevation, affiliation, component, owner, genre,  manufacturer, brand, family, credit, depth,\n",
    "classification, collection, species, command, nationality, currency, range, affiliate,\n",
    "birthDate, ranking, capacity, birthPlace, person, creator, operator, religion, education,\n",
    "requirement, director, sales, continent, organisation\n",
    "Do not use any column names aside from these.\n",
    "\n",
    "Output must be in valid JSON like the following example {\"colnames\" : [\"col1\", \"col2\"]}\n",
    "\n",
    "Given the following relational table:\n",
    "\"\"\"\n",
    "\n",
    "trues = []\n",
    "preds = []\n",
    "\n",
    "true_path = \"npy/trues/\"\n",
    "pred_path = \"npy/preds/\" \n",
    "if not os.path.exists(true_path):\n",
    "    os.makedirs(true_path)\n",
    "if not os.path.exists(pred_path):\n",
    "    os.makedirs(pred_path)\n",
    "\n",
    "for tabledir in tqdm(directory_indexs):\n",
    "    filenames = os.listdir(tabledir)\n",
    "    real_cols = []\n",
    "    pred_cols = []\n",
    "    mismatch = 0\n",
    "    error_num = 0\n",
    "    file_cnt = 0\n",
    "    for filename in filenames:\n",
    "        with open(tabledir + '/' + filename) as f:\n",
    "            linelist = f.readlines()\n",
    "            colnames = linelist[0][:-1].split(',')\n",
    "            if column_type == \"single\" and len(colnames) != 1:\n",
    "                continue\n",
    "            if column_type == \"multi\" and len(colnames) <= 1:\n",
    "                continue\n",
    "            lines = ''.join(linelist[1:rows_num+1])#.replace(',',';')\n",
    "            content = f\"{template_context} \\n {lines} Guess the column names for the whole table. There are only {len(colnames)} columns in the table.\"\n",
    "            if column_type == \"multi\":\n",
    "                content += \"It is possible for multiple columns to have the same name.\\n\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "            You are a database expert who can make general predictions for missing column values in database tables, and the predicted column names are within the required candidate set. All output must be in valid JSON. Don't add explanation beyond the JSON.\n",
    "            Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "            \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": content},\n",
    "            ]\n",
    "\n",
    "            prompt = pipeline.tokenizer.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            terminators = [\n",
    "                pipeline.tokenizer.eos_token_id,\n",
    "                pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            ]\n",
    "\n",
    "            outputs = pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                eos_token_id=terminators,\n",
    "                pad_token_id=pipeline.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "            try:\n",
    "                jslist = loads(outputs[0][\"generated_text\"][len(prompt):])\n",
    "                \n",
    "                if len(jslist[\"colnames\"]) == len(colnames):\n",
    "                    real_cols += colnames\n",
    "                    items = jslist[\"colnames\"]\n",
    "                    for item in items:\n",
    "                        pred_cols.append(canonical_header(item))\n",
    "                else:\n",
    "                    mismatch += 1\n",
    "                    if not ignore_mismatch:\n",
    "                        real_cols += colnames\n",
    "                        pred_cols += [\"???\"] * len(colnames)\n",
    "                file_cnt += 1\n",
    "            except JSONDecodeError as e:\n",
    "                # below two lines for debug\n",
    "                # print(\"json load failed:\", filename, outputs[0][\"generated_text\"][len(prompt):])\n",
    "                # print(\"error:\", str(e))\n",
    "                error_num += 1\n",
    "        if file_cnt == file_num:\n",
    "            break\n",
    "\n",
    "    print(len(pred_cols), len(real_cols), mismatch, error_num)\n",
    "    with open(f'npy/trues/{column_type}_{tabledir}_true.npy', 'wb') as f:\n",
    "        np.save(f, np.array(real_cols, dtype='<U14'))\n",
    "    with open(f'npy/preds/{column_type}_{tabledir}_pred.npy', 'wb') as f:\n",
    "        np.save(f, np.array(pred_cols, dtype='<U14'))\n",
    "    trues += real_cols\n",
    "    preds += pred_cols\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181 1181\n",
      "Results are successfully written into results/multi_overall_8B.json and results/multi_report_8B.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cs.wisc.edu/u/e/t/ethanfang/private/CS839/f1_llama.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_report= pd.concat([df_report, pd.DataFrame(report[t],index=[0])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_path = \"results/\" \n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "print(len(preds), len(trues))\n",
    "overall, report = report_gen(preds,trues)\n",
    "with open(f\"results/{column_type}_overall_{model_size}.json\",\"w\") as f:\n",
    "    json.dump(overall,f)\n",
    "report.to_csv(f'results/{column_type}_report_{model_size}.csv', index=False)\n",
    "print(f\"Results are successfully written into results/{column_type}_overall_{model_size}.json and results/{column_type}_report_{model_size}.csv\")\n",
    "# \"\"\"correct = 0\n",
    "# total = 0\n",
    "# for real,pred in zip(real_cols, pred_cols):\n",
    "#     for r,p in zip(real,pred):\n",
    "#         total += 1\n",
    "#         if r == (p[0].lower() + p[1:]):\n",
    "#             correct += 1\n",
    "# print(f'Accuracy: {correct/total}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
