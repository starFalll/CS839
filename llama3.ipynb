{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install transformers>=4.32.0 optimum>=1.12.0\n",
    "! pip3 install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model type\n",
    "# model_id = \"TechxGenus/Meta-Llama-3-70B-Instruct-GPTQ\"\n",
    "# model_size = \"70B\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_size = \"8B\"\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_size = \"7B_adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom parameters\n",
    "column_type = \"single\"                             # single/multi, like for single column type, We only read tables with only one column\n",
    "directory_indexs = [\"K0\", \"K1\", \"K2\", \"K3\", \"K4\"] # directory used for iterate\n",
    "file_num = 100                                    # for each directory, read file_num of column_type files\n",
    "rows_num = 20                                     # for each table, how many rows we need read for prompt\n",
    "max_new_tokens = 4096                             # max token for LLM\n",
    "# ignore_mismatch = False                           # if throw out mismatch\n",
    "gpu_device = \"auto\"                               # 8B needs specify the index of GPU like 1 or 2, 70B use \"auto\"\n",
    "enable_adapter = True\n",
    "adapter_id = \"sadpineapple/llama2-7b-chat-adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/s/d/sdam/miniconda3/envs/llama2/lib/python3.12/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
     ]
    }
   ],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             device_map=gpu_device,\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "if enable_adapter:\n",
    "    model.load_adapter(adapter_id)\n",
    "    # model.enable_adapters()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_id,\n",
    "#     # model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device=gpu_device,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "            You are a database expert who can make general predictions for missing column values in database tables, and the predicted column names are within the required candidate set. All output must be in valid JSON. Don't add explanation beyond the JSON.\n",
    "            Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "            \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": \"\"\"\n",
    "Column Names are limited to the following:\n",
    "name, description, team, type, age, location, year, city, rank, status, state, category,\n",
    "weight, code, club, artist, result, position, country, notes, class, company, album, symbol,\n",
    "address, duration, format, county, day, gender, industry, language, sex, product, jockey,\n",
    "region, area, service, teamName, order, isbn, fileSize, grades, publisher, plays, origin,\n",
    "elevation, affiliation, component, owner, genre,  manufacturer, brand, family, credit, depth,\n",
    "classification, collection, species, command, nationality, currency, range, affiliate,\n",
    "birthDate, ranking, capacity, birthPlace, person, creator, operator, religion, education,\n",
    "requirement, director, sales, continent, organisation\n",
    "Do not use any column names aside from these.\n",
    "\n",
    "Output must be in valid JSON like the following example {\"colnames\" : [\"col1\", \"col2\"]}\n",
    "\n",
    "Given the following relational table:\n",
    "\n",
    "IDI1,isopentenyl-diphosphate delta isomerase..isopentenyl-diphosphate delta isomerase.\n",
    "FDFT1,farnesyl-diphosphate farnesyltransferase 1.DGPT.farnesyl-diphosphate farnesyltransferase 1.\n",
    "FDPS,\"farnesyl diphosphate synthase (farnesyl pyrophosphate synthetase, dimethylallyltranstransferase, geranyltranstransferase).FPS..\"\n",
    "HPCAL1,hippocalcin-like 1.BDR1; HLP2; VILIP-3.hippocalcin-like 1.\n",
    "LRP8,\"low density lipoprotein receptor-related protein 8, apolipoprotein e receptor.APOER2; HSZ75190.low density lipoprotein receptor-related protein 8 isoform 3 precursor.\"\n",
    "AP2S1,\"adaptor-related protein complex 2, sigma 1 subunit.AP17; CLAPS2; AP17-DELTA.adaptor-related protein complex 2, sigma 1 subunit isoform AP17delta.\"\n",
    "SQLE,squalene epoxidase..squalene monooxygenase.\n",
    "HPCA,hippocalcin.BDR2.hippocalcin.\n",
    ",\"tubulin, beta, 2.\"\n",
    "AP3S1,\"adaptor-related protein complex 3, sigma 1 subunit.CLAPS3; Sigma3A.adaptor-related protein complex 3, sigma 1 subunit.\"\n",
    "AP1S2,\"adaptor-related protein complex 1, sigma 2 subunit.DC22; SIGMA1B; MGC:1902.adaptor-related protein complex 1 sigma 2 subunit.\"\n",
    "TUBA1,\"tubulin, alpha 1 (testis specific).FLJ30169; H2-ALPHA.tubulin, alpha 1.\"\n",
    "AP2A1,\"adaptor-related protein complex 2, alpha 1 subunit.ADTAA; CLAPA1; AP2-ALPHA.adaptor-related protein complex 2, alpha 1 subunit isoform 2.\"\n",
    "SEC24D,\"SEC24 related gene family, member D (S. cerevisiae).KIAA0755.Sec24-related protein D.\"\n",
    "                 \n",
    "Guess the column names for the whole table. There are only 2 columns in the table. Give only 1 answer for each column.\n",
    "                 \"\"\"},\n",
    "            ]\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=4096,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "test = pipe(messages)[0]['generated_text']\n",
    "print(test[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from json import loads, JSONDecodeError\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from npy_postprocess import canonical_header\n",
    "from f1_llama import report_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_context = \"\"\"\n",
    "Column Names are limited to the following:\n",
    "name, description, team, type, age, location, year, city, rank, status, state, category,\n",
    "weight, code, club, artist, result, position, country, notes, class, company, album, symbol,\n",
    "address, duration, format, county, day, gender, industry, language, sex, product, jockey,\n",
    "region, area, service, teamName, order, isbn, fileSize, grades, publisher, plays, origin,\n",
    "elevation, affiliation, component, owner, genre,  manufacturer, brand, family, credit, depth,\n",
    "classification, collection, species, command, nationality, currency, range, affiliate,\n",
    "birthDate, ranking, capacity, birthPlace, person, creator, operator, religion, education,\n",
    "requirement, director, sales, continent, organisation\n",
    "Do not use any column names aside from these.\n",
    "\"\"\"\n",
    "\n",
    "single_context = template_context + \"\"\"\n",
    "Output must be in valid JSON like the following example {\"column\" : \"col1\"}. Give only 1 prediction. Do NOT add any explanation beyond the JSON.\n",
    "\n",
    "Given the following column values in a relational table:\"\"\"\n",
    "\n",
    "multi_context = template_context + \"\"\"\n",
    "Output must be in valid JSON like the following example {\"colnames\" : [\"col1\", \"col2\"]}\n",
    "\n",
    "Given the following relational table:\n",
    "\"\"\"\n",
    "\n",
    "sysprompt = {\"role\": \"system\", \"content\": \"\"\"\n",
    "            You are a database expert who can make general predictions for missing column values in database tables, and the predicted column names are within the required candidate set. All output must be in valid JSON. Don't add explanation beyond the JSON.\n",
    "            Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "            \"\"\"}\n",
    "\n",
    "trues = []\n",
    "preds = []\n",
    "\n",
    "true_path = \"npy/trues/\"\n",
    "pred_path = \"npy/preds/\" \n",
    "if not os.path.exists(true_path):\n",
    "    os.makedirs(true_path)\n",
    "if not os.path.exists(pred_path):\n",
    "    os.makedirs(pred_path)\n",
    "\n",
    "for tabledir in tqdm(directory_indexs):\n",
    "    filenames = os.listdir(tabledir)\n",
    "    real_cols = []\n",
    "    pred_cols = []\n",
    "    mismatch = 0\n",
    "    error_num = 0\n",
    "    #file_cnt = 0\n",
    "    for filename in filenames[0:file_num]: #moved file_num here\n",
    "        fullpath = tabledir + '/' + filename\n",
    "        with open(fullpath) as f:\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "                top_k=40,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "            linelist = f.readlines()\n",
    "            colnames = linelist[0][:-1].split(',')\n",
    "            real_cols += colnames\n",
    "            #if column_type == \"single\" and len(colnames) != 1:\n",
    "            #    continue\n",
    "            #if column_type == \"multi\" and len(colnames) <= 1:\n",
    "            #    continue\n",
    "            if column_type == \"multi\":\n",
    "                lines = ''.join(linelist[1:rows_num+1])#.replace(',',';')\n",
    "                content = f\"{multi_context} \\n {lines} Guess the column names for the whole table. There are only {len(colnames)} columns in the table. It is possible for multiple columns to have the same name.\\n\"\n",
    "                messages = [sysprompt, {\"role\": \"user\", \"content\": content},]\n",
    "                try:\n",
    "                    jslist = loads(pipe(messages)[0]['generated_text'][-1]['content'])\n",
    "                    \n",
    "                    if len(jslist[\"colnames\"]) == len(colnames):\n",
    "                        #real_cols += colnames #commented because I already have it above\n",
    "                        items = jslist[\"colnames\"]\n",
    "                        for item in items:\n",
    "                            pred_cols.append(canonical_header(item))\n",
    "                    else:\n",
    "                        mismatch += 1\n",
    "                        pred_cols += [\"???\"] * len(colnames) #removed ignore_mismatch\n",
    "                except JSONDecodeError as e:\n",
    "                    # below two lines for debug\n",
    "                    # print(\"json load failed:\", filename, outputs[0][\"generated_text\"][len(prompt):])\n",
    "                    # print(\"error:\", str(e))\n",
    "                    pred_cols += [\"???\"] * len(colnames)\n",
    "                    error_num += 1\n",
    "            elif column_type == \"single\":\n",
    "                df = pd.read_csv(fullpath).astype(str)\n",
    "                for col in df.columns:\n",
    "                    content = f'{single_context} {', '.join(df[col][0:rows_num])}\\nGuess the column name'\n",
    "                    messages = [sysprompt, {\"role\": \"user\", \"content\": content},]\n",
    "                    try:\n",
    "                        cpred = loads(pipe(messages)[0]['generated_text'][-1]['content'])[\"column\"]\n",
    "                        pred_cols.append(canonical_header(cpred))\n",
    "                    except JSONDecodeError as e:\n",
    "                        # below two lines for debug\n",
    "                        # print(\"json load failed:\", filename, outputs[0][\"generated_text\"][len(prompt):])\n",
    "                        # print(\"error:\", str(e))\n",
    "                        pred_cols.append(\"???\")\n",
    "                        error_num += 1\n",
    "        #file_cnt += 1\n",
    "        #if file_cnt == file_num:\n",
    "        #    break\n",
    "\n",
    "    print(len(pred_cols), len(real_cols), mismatch, error_num)\n",
    "    with open(f'npy/trues/{column_type}_{tabledir}_true.npy', 'wb') as f:\n",
    "        np.save(f, np.array(real_cols, dtype='<U14'))\n",
    "    with open(f'npy/preds/{column_type}_{tabledir}_pred.npy', 'wb') as f:\n",
    "        np.save(f, np.array(pred_cols, dtype='<U14'))\n",
    "    trues += real_cols\n",
    "    preds += pred_cols\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"results/\" \n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "print(len(preds), len(trues))\n",
    "overall, report = report_gen(preds,trues)\n",
    "with open(f\"results/{column_type}_overall_{model_size}.json\",\"w\") as f:\n",
    "    json.dump(overall,f)\n",
    "report.to_csv(f'results/{column_type}_report_{model_size}.csv', index=False)\n",
    "print(f\"Results are successfully written into results/{column_type}_overall_{model_size}.json and results/{column_type}_report_{model_size}.csv\")\n",
    "# \"\"\"correct = 0\n",
    "# total = 0\n",
    "# for real,pred in zip(real_cols, pred_cols):\n",
    "#     for r,p in zip(real,pred):\n",
    "#         total += 1\n",
    "#         if r == (p[0].lower() + p[1:]):\n",
    "#             correct += 1\n",
    "# print(f'Accuracy: {correct/total}')\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama)",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
